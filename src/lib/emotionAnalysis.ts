// New Gemini API functions for emotion analysis workflow
// These functions complement the existing gemini.ts functions

import { GoogleGenerativeAI } from '@google/generative-ai';
import { Participant, EmotionPoint, Anomaly } from './gemini';

const API_KEY = process.env.NEXT_PUBLIC_GEMINI_API_KEY || 'YOUR_API_KEY_HERE';
const genAI = new GoogleGenerativeAI(API_KEY);

/**
 * Helper: Convert File to Generative Part
 */
async function fileToGenerativePart(file: File) {
    const base64 = await fileToBase64(file);
    return {
        inlineData: {
            data: base64.split(',')[1],
            mimeType: file.type
        }
    };
}

/**
 * Helper: File to Base64
 */
function fileToBase64(file: File): Promise<string> {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = () => resolve(reader.result as string);
        reader.onerror = reject;
        reader.readAsDataURL(file);
    });
}

/**
 * Get Event Summary - Brief description of what happened in the video
 */
export async function getEventSummary(
    videoFile: File,
    participants: Participant[]
): Promise<string> {
    try {
        console.log('ğŸ“ Getting event summary...');
        const model = genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });
        const videoData = await fileToGenerativePart(videoFile);

        const participantContext = participants.map(p =>
            `${p.name} (${p.age} ×©× ×™×, ${p.role})`
        ).join(', ');

        const prompt = `
× ×ª×— ××ª ×”×¡ ×¨×˜×•×Ÿ ×•×›×ª×•×‘ ×¡×™×›×•× ×§×¦×¨ (2-3 ××©×¤×˜×™×) ×©×œ ××” ×§×¨×”.

××©×ª×ª×¤×™×: ${participantContext}

×”×ª××§×“ ×‘:
- ××” ×”××™×¨×•×¢ ×”××¨×›×–×™ ×©×§×¨×”?
- ××™ ×”×™×• ×”××¢×•×¨×‘×™×?
- ××” ×”×™×” ×”×˜×•×Ÿ ×”×›×œ×œ×™ (×—×™×•×‘×™/×©×œ×™×œ×™/× ×™×˜×¨×œ×™)?

×”×—×–×¨ ×˜×§×¡×˜ ×¤×©×•×˜, ×œ×œ× JSON.
    `;

        const result = await model.generateContent([prompt, videoData]);
        const summary = result.response.text().trim();
        console.log('âœ… Event summary:', summary);
        return summary;
    } catch (error: any) {
        console.error('âŒ Event summary error:', error);
        return '×œ× × ×™×ª×Ÿ ×œ×¡×›× ××ª ×”××™×¨×•×¢. ×× × × ×¡×” ×©×•×‘.';
    }
}

/**
 * Generate Emotion Timeline - Track emotions 1-5 for each participant
 */
export async function generateEmotionTimeline(
    videoFile: File,
    participants: Participant[],
    customPrompts: any
): Promise<EmotionPoint[]> {
    try {
        console.log('ğŸ“ˆ Generating emotion timeline...');
        const model = genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });
        const videoData = await fileToGenerativePart(videoFile);

        const participantContext = participants.map(p =>
            `${p.name} (ID: ${p.id}, ${p.age} ×©× ×™×, ${p.role})`
        ).join('\\n');

        const prompt = `
× ×ª×— ××ª ×”×¡×¨×˜×•×Ÿ ×•×¦×•×¨ ×’×¨×£ ×¨×’×©×•×ª ×œ×›×œ ××©×ª×ª×£.

××©×ª×ª×¤×™×:
${participantContext}

${customPrompts.sections.identity}

×—×©×•×‘: ×‘× ×” ×˜×‘×œ×ª × ×§×•×“×•×ª ×¨×’×© ×¢×œ ×¦×™×¨ ×–××Ÿ.
×œ×›×œ × ×§×•×“×” ×–××Ÿ ×—×©×•×‘×” (×›×œ 10-15 ×©× ×™×•×ª, ××• ×›×©×™×© ×©×™× ×•×™ ××©××¢×•×ª×™):
- timestamp ×‘×¤×•×¨××˜ MM:SS
- participantId (×”×©×ª××© ×‘-ID ×”××“×•×™×§ ××”×¨×©×™××” ×œ××¢×œ×”)
- emotionLevel: ××¡×¤×¨ ×‘×™×Ÿ 1-5
  1 = ×××•×“ ×©×œ×™×œ×™ (×›×¢×¡, ×¢×¦×‘, ×¤×—×“)
  2 = ×©×œ×™×œ×™ ×§×œ
  3 = × ×™×˜×¨×œ×™
  4 = ×—×™×•×‘×™
  5 = ×××•×“ ×—×™×•×‘×™ (×©××—×”, ×”×ª×œ×”×‘×•×ª)
- event: ×ª×™××•×¨ ×§×¦×¨ ×©×œ ××” ×§×•×¨×” ×‘×¨×’×¢ ×–×”

×”×—×–×¨ JSON ×‘×¤×•×¨××˜ ×”×‘×:
[
  {
    "timestamp": "00:15",
    "participantId": "person_1",
    "emotionLevel": 3,
    "event": "×ª×™××•×¨"
  }
]

×–×”×” ×œ×¤×—×•×ª 8-12 × ×§×•×“×•×ª ×–××Ÿ ×œ×›×œ ××©×ª×ª×£.
    `;

        const result = await model.generateContent([prompt, videoData]);
        const response = result.response.text();
        console.log('âœ… Got emotion timeline response');

        const cleanResponse = response.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
        const parsed = JSON.parse(cleanResponse);

        // Convert timestamp to seconds and add participant names
        const emotionPoints: EmotionPoint[] = parsed.map((point: any) => {
            const [minutes, seconds] = point.timestamp.split(':').map(Number);
            const participant = participants.find(p => p.id === point.participantId);

            return {
                timestamp: point.timestamp,
                timestampSeconds: minutes * 60 + seconds,
                participantId: point.participantId,
                participantName: participant?.name || '×œ× ×™×“×•×¢',
                emotionLevel: point.emotionLevel,
                event: point.event
            };
        });

        return emotionPoints;
    } catch (error: any) {
        console.error('âŒ Emotion timeline error:', error);
        alert(`×©×’×™××” ×‘×™×¦×™×¨×ª ×’×¨×£ ×¨×’×©×•×ª: ${error?.message || '×©×’×™××” ×œ× ×™×“×•×¢×”'}`);
        return [];
    }
}

/**
 * Analyze Anomalies - Deep analysis of events with emotion < 2
 */
export async function analyzeAnomalies(
    videoFile: File,
    emotionTimeline: EmotionPoint[],
    participants: Participant[],
    customPrompts: any
): Promise<Anomaly[]> {
    // Filter anomalies (emotion < 2)
    const anomalyPoints = emotionTimeline.filter(p => p.emotionLevel < 2);

    if (anomalyPoints.length === 0) {
        console.log('âœ… No anomalies detected (all emotions >= 2)');
        return [];
    }

    try {
        console.log(`ğŸ” Analyzing ${anomalyPoints.length} anomalies...`);
        const model = genAI.getGenerativeModel({ model: 'gemini-2.5-pro' });
        const videoData = await fileToGenerativePart(videoFile);

        const anomalyContext = anomalyPoints.map(p =>
            `- ${p.timestamp}: ${p.participantName} (×¨×’×©: ${p.emotionLevel}/5) - ${p.event}`
        ).join('\\n');

        const prompt = `
×–×•×”×• ×”××™×¨×•×¢×™× ×”×—×¨×™×’×™× ×”×‘××™× ×‘×¡×¨×˜×•×Ÿ (×¨××ª ×¨×’×© ××ª×—×ª ×œ-2):

${anomalyContext}

${customPrompts.sections.forensic}
${customPrompts.sections.psychology}

×œ×›×œ ××™×¨×•×¢ ×—×¨×™×’:
1. ×¦×¤×” ×‘×§×˜×¢ ×”×¡×¤×¦×™×¤×™ ×‘×–××Ÿ ×”× ×ª×•×Ÿ
2. × ×ª×— ××” ×’×¨× ×œ×¨×’×© ×”×©×œ×™×œ×™
3. ×”×¢×¨×š ××ª ×—×•××¨×ª ×”××™×¨×•×¢
4. ×ª×Ÿ ×”××œ×¦×•×ª ×¡×¤×¦×™×¤×™×•×ª

×”×—×–×¨ JSON:
[
  {
    "timestamp": "MM:SS",
    "participantId": "person_X",
    "emotionLevel": 1,
    "description": "× ×™×ª×•×— ××¤×•×¨×˜ ×©×œ ××” ×§×¨×” ×•×œ××”",
    "severity": "low" | "medium" | "high"
  }
]
    `;

        const result = await model.generateContent([prompt, videoData]);
        const response = result.response.text();
        console.log('âœ… Got anomaly analysis');

        const cleanResponse = response.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
        const parsed = JSON.parse(cleanResponse);

        // Add participant names and timestamp in seconds
        const anomalies: Anomaly[] = parsed.map((anomaly: any) => {
            const [minutes, seconds] = anomaly.timestamp.split(':').map(Number);
            const participant = participants.find(p => p.id === anomaly.participantId);

            return {
                ...anomaly,
                timestampSeconds: minutes * 60 + seconds,
                participantName: participant?.name || '×œ× ×™×“×•×¢×¢'
            };
        });

        return anomalies;
    } catch (error: any) {
        console.error('âŒ Anomaly analysis error:', error);
        alert(`×©×’×™××” ×‘× ×™×ª×•×— ××™×¨×•×¢×™× ×—×¨×™×’×™×: ${error?.message || '×©×’×™××” ×œ× ×™×“×•×¢×”'}`);
        return [];
    }
}
